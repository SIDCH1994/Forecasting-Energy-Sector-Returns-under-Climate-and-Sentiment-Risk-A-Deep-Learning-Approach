{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f741eef7-068c-4322-8cdd-a714d91bbe4b",
   "metadata": {},
   "source": [
    "## Mega Dataset Ceration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5ae6d7-d3f3-4872-8e58-f8e39b0dc595",
   "metadata": {},
   "source": [
    "### 1. Setup Paths & Load Backbone\n",
    "\n",
    "**Explanation:** We define our directories and load the \"Backbone\" dataset (stock_senti_engineered_imputed_2.csv). This file contains our 7 tickers and the dates. It dictates the \"shape\" of the final dataset. We verify the initial row count so we can check for duplicates later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21718500-fcb3-45b4-92e4-bc4b9e81759c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Backbone: D:\\MS_Data_Science_Thesis\\Data_Cleaning\\Semi_Clean_Datasets\\stock_senti_engineered_imputed_2.csv\n",
      "Initial Backbone Rows: 24052\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>volume</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj close</th>\n",
       "      <th>ticker</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>n_articles</th>\n",
       "      <th>...</th>\n",
       "      <th>MA7</th>\n",
       "      <th>MA50</th>\n",
       "      <th>RSI</th>\n",
       "      <th>Log_Returns</th>\n",
       "      <th>Close_to_MA7</th>\n",
       "      <th>Close_to_MA50</th>\n",
       "      <th>Sent_MA7</th>\n",
       "      <th>Sent_MA30</th>\n",
       "      <th>Sent_Vol7</th>\n",
       "      <th>sentiment_imputed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-04-27</td>\n",
       "      <td>23420500.0</td>\n",
       "      <td>43.833202</td>\n",
       "      <td>44.587894</td>\n",
       "      <td>41.363297</td>\n",
       "      <td>43.863693</td>\n",
       "      <td>30.670647</td>\n",
       "      <td>COP</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>43.797264</td>\n",
       "      <td>39.971154</td>\n",
       "      <td>74.139756</td>\n",
       "      <td>-0.013122</td>\n",
       "      <td>1.001517</td>\n",
       "      <td>1.097384</td>\n",
       "      <td>-0.162053</td>\n",
       "      <td>0.013629</td>\n",
       "      <td>0.343736</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-04-28</td>\n",
       "      <td>22939200.0</td>\n",
       "      <td>43.901810</td>\n",
       "      <td>44.824215</td>\n",
       "      <td>43.772217</td>\n",
       "      <td>44.633633</td>\n",
       "      <td>31.209005</td>\n",
       "      <td>COP</td>\n",
       "      <td>0.084899</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>44.005268</td>\n",
       "      <td>40.102730</td>\n",
       "      <td>74.085707</td>\n",
       "      <td>0.017401</td>\n",
       "      <td>1.014279</td>\n",
       "      <td>1.112982</td>\n",
       "      <td>-0.149924</td>\n",
       "      <td>-0.011301</td>\n",
       "      <td>0.351810</td>\n",
       "      <td>0.084899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-04-29</td>\n",
       "      <td>17990400.0</td>\n",
       "      <td>44.976677</td>\n",
       "      <td>45.853340</td>\n",
       "      <td>44.976677</td>\n",
       "      <td>45.052910</td>\n",
       "      <td>31.502172</td>\n",
       "      <td>COP</td>\n",
       "      <td>-0.020716</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>44.190402</td>\n",
       "      <td>40.252144</td>\n",
       "      <td>71.624744</td>\n",
       "      <td>0.009350</td>\n",
       "      <td>1.019518</td>\n",
       "      <td>1.119267</td>\n",
       "      <td>-0.152884</td>\n",
       "      <td>-0.034716</td>\n",
       "      <td>0.350423</td>\n",
       "      <td>-0.020716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date      volume       open       high        low      close  \\\n",
       "0 2010-04-27  23420500.0  43.833202  44.587894  41.363297  43.863693   \n",
       "1 2010-04-28  22939200.0  43.901810  44.824215  43.772217  44.633633   \n",
       "2 2010-04-29  17990400.0  44.976677  45.853340  44.976677  45.052910   \n",
       "\n",
       "   adj close ticker  sentiment_score  n_articles  ...        MA7       MA50  \\\n",
       "0  30.670647    COP         0.000000         0.0  ...  43.797264  39.971154   \n",
       "1  31.209005    COP         0.084899         1.0  ...  44.005268  40.102730   \n",
       "2  31.502172    COP        -0.020716         4.0  ...  44.190402  40.252144   \n",
       "\n",
       "         RSI  Log_Returns  Close_to_MA7  Close_to_MA50  Sent_MA7  Sent_MA30  \\\n",
       "0  74.139756    -0.013122      1.001517       1.097384 -0.162053   0.013629   \n",
       "1  74.085707     0.017401      1.014279       1.112982 -0.149924  -0.011301   \n",
       "2  71.624744     0.009350      1.019518       1.119267 -0.152884  -0.034716   \n",
       "\n",
       "   Sent_Vol7  sentiment_imputed  \n",
       "0   0.343736           0.000000  \n",
       "1   0.351810           0.084899  \n",
       "2   0.350423          -0.020716  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Define Directories ---\n",
    "# Directory where your 8 semi-clean files live\n",
    "semi_clean_dir = r\"D:\\MS_Data_Science_Thesis\\Data_Cleaning\\Semi_Clean_Datasets\"\n",
    "\n",
    "# Final Output Directory\n",
    "final_dir = r\"D:\\MS_Data_Science_Thesis\\Data_Cleaning\\Master_Dataset\"\n",
    "os.makedirs(final_dir, exist_ok=True)\n",
    "\n",
    "# --- Load the Backbone (Stock + Sentiment) ---\n",
    "backbone_path = os.path.join(semi_clean_dir, \"stock_senti_engineered_imputed_2.csv\")\n",
    "print(f\"Loading Backbone: {backbone_path}\")\n",
    "\n",
    "df_final = pd.read_csv(backbone_path)\n",
    "df_final['date'] = pd.to_datetime(df_final['date'])\n",
    "\n",
    "# Store initial row count for integrity check\n",
    "initial_rows = len(df_final)\n",
    "print(f\"Initial Backbone Rows: {initial_rows}\")\n",
    "df_final.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11b2d08-91c5-4b53-b3ed-2cf3ebdfb577",
   "metadata": {},
   "source": [
    "### 2. Define Macro File Map\n",
    "\n",
    "**Explanation:** We create a Python dictionary mapping a \"Readable Name\" to the specific \"Filename\" you provided in your screenshot. This makes the next step (the loop) cleaner and less error-prone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f56e9b42-bd1f-40b3-bc52-f534ce3b3823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 7 macro files to merge.\n"
     ]
    }
   ],
   "source": [
    "# --- Define the Macro Files to Merge ---\n",
    "# Dictionary mapping: {Name_for_Log : Filename}\n",
    "# These match the filenames provided in your screenshot\n",
    "macro_files = {\n",
    "    \"Oil\": \"oil_engineered_3.csv\",\n",
    "    \"Gas\": \"Gas_Engineered_4.csv\",\n",
    "    \"VIX\": \"VIX_engineered_5.csv\",\n",
    "    \"XLE\": \"XLE_engineered_6.csv\",\n",
    "    \"Weather\": \"HDDCDD_engineered_7.csv\",\n",
    "    \"Hurricane\": \"HUDRAT2_engineered_8.csv\",\n",
    "    \"Carbon\": \"carbon_engineered_9.csv\"\n",
    "}\n",
    "\n",
    "print(f\"Identified {len(macro_files)} macro files to merge.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30106f7e-86f7-4f40-aceb-44e0f2dfa6dd",
   "metadata": {},
   "source": [
    "### 3. The Iterative Merge Loop\n",
    "\n",
    "**Explanation:** This is the engine. We loop through each file in the list.\n",
    "\n",
    "- **Left Join:** We keep all stock rows. We match macro data based on date.\n",
    "\n",
    "- **validate='many_to_one':** This is a safety feature. It ensures that the macro file (the \"one\" side) doesn't have duplicate dates. If it did, it would explode your dataset size.\n",
    "\n",
    "- **Integrity Check:** After every merge, we check if the row count changed. If it did, something went wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea0d1c5f-5862-4a7f-858d-ed6a28032c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging Oil data...\n",
      " -> Successfully merged Oil. Added 6 columns.\n",
      "Merging Gas data...\n",
      " -> Successfully merged Gas. Added 6 columns.\n",
      "Merging VIX data...\n",
      " -> Successfully merged VIX. Added 6 columns.\n",
      "Merging XLE data...\n",
      " -> Successfully merged XLE. Added 6 columns.\n",
      "Merging Weather data...\n",
      " -> Successfully merged Weather. Added 8 columns.\n",
      "Merging Hurricane data...\n",
      " -> Successfully merged Hurricane. Added 1 columns.\n",
      "Merging Carbon data...\n",
      " -> Successfully merged Carbon. Added 6 columns.\n",
      "--------------------------------------------------\n",
      "Merge sequence complete.\n"
     ]
    }
   ],
   "source": [
    "# --- The Iterative Mega Merge ---\n",
    "for name, filename in macro_files.items():\n",
    "    file_path = os.path.join(semi_clean_dir, filename)\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"ERROR: Could not find {filename}\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"Merging {name} data...\")\n",
    "    \n",
    "    # Load Macro\n",
    "    df_macro = pd.read_csv(file_path)\n",
    "    df_macro['date'] = pd.to_datetime(df_macro['date'])\n",
    "    \n",
    "    # MERGE (Left Join on Date)\n",
    "    # validate='many_to_one' ensures we don't accidentally explode rows\n",
    "    df_final = pd.merge(df_final, df_macro, on='date', how='left', validate='many_to_one')\n",
    "    \n",
    "    # Check if rows changed (Safety Check)\n",
    "    if len(df_final) != initial_rows:\n",
    "        print(f\"WARNING: Row count changed after merging {name}! Check for duplicates.\")\n",
    "    else:\n",
    "        # Print which new columns were added (excluding 'date')\n",
    "        new_cols = list(df_macro.columns.difference(['date']))\n",
    "        print(f\" -> Successfully merged {name}. Added {len(new_cols)} columns.\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Merge sequence complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c633bdf-f737-43f2-82a7-878d8d92ac13",
   "metadata": {},
   "source": [
    "### 4. Sanity Checks (Panel Verification)\n",
    "\n",
    "**Explanation:** We verify the data quality before saving.\n",
    "- **Row Count:** Must equal the initial count.\n",
    "\n",
    "- **Null Check:** Since we imputed everything, this should be 0.\n",
    "\n",
    "- **Panel Check:** We pick a random date and print rows for multiple tickers (e.g., XOM, CVX). They should have different stock prices but identical Oil/Gas/VIX values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "25f11109-8881-493a-8e4c-cc97b7daf7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FINAL SANITY CHECKS ---\n",
      "1. Final Row Count: 24052 (Should be 24052)\n",
      "2. Null Check: WARNING (Found missing values):\n",
      "Oil_Log_Return        21\n",
      "Oil_RSI               91\n",
      "Oil_MA7               42\n",
      "Oil_MA50             343\n",
      "Oil_Vol7             105\n",
      "Gas_Log_Return         7\n",
      "Gas_RSI               91\n",
      "Gas_MA7               42\n",
      "Gas_MA30             203\n",
      "Gas_Vol7              49\n",
      "VIX_Log_Return         7\n",
      "VIX_Diff               7\n",
      "VIX_RSI               91\n",
      "VIX_MA50             343\n",
      "VIX_to_MA50          343\n",
      "XLE_Log_Return         7\n",
      "XLE_RSI               91\n",
      "XLE_MA7               42\n",
      "XLE_MA50             343\n",
      "XLE_Vol7              49\n",
      "HDD_MA7               42\n",
      "CDD_MA7               42\n",
      "Total_DD_MA7          42\n",
      "Total_DD_MA30        203\n",
      "Weather_Shock        203\n",
      "Carbon_Log_Return      7\n",
      "Carbon_RSI            91\n",
      "Carbon_MA7            42\n",
      "Carbon_MA50          343\n",
      "Carbon_Vol7           49\n",
      "dtype: int64\n",
      "\n",
      "3. Panel Check for date 2010-09-17 (Macros should be identical, Ticker/Close different):\n",
      "      ticker      close  Oil_Price  Gas_Price  VIX_Close  Carbon_Price\n",
      "100      COP  42.133236      73.63       4.11      22.01         15.31\n",
      "3543     CVX  78.459999      73.63       4.11      22.01         15.31\n",
      "6986     EOG  44.555000      73.63       4.11      22.01         15.31\n",
      "10429    HAL  30.959999      73.63       4.11      22.01         15.31\n",
      "13823    HES  54.970001      73.63       4.11      22.01         15.31\n",
      "17266    OXY  73.004280      73.63       4.11      22.01         15.31\n",
      "20709    XOM  60.779999      73.63       4.11      22.01         15.31\n"
     ]
    }
   ],
   "source": [
    "# --- Final Sanity Checks ---\n",
    "print(\"--- FINAL SANITY CHECKS ---\")\n",
    "\n",
    "# 1. Row Count Integrity\n",
    "print(f\"1. Final Row Count: {len(df_final)} (Should be {initial_rows})\")\n",
    "\n",
    "# 2. Null Value Check (Should be 0 for all columns)\n",
    "nulls = df_final.isna().sum()\n",
    "if nulls.sum() == 0:\n",
    "    print(\"2. Null Check: PASSED (No missing values in the entire dataset!)\")\n",
    "else:\n",
    "    print(\"2. Null Check: WARNING (Found missing values):\")\n",
    "    print(nulls[nulls > 0])\n",
    "\n",
    "# 3. Panel Structure Verification\n",
    "# We check a specific date to ensure macros are repeated correctly across tickers\n",
    "if len(df_final) > 100:\n",
    "    check_date = df_final['date'].iloc[100] # Pick a random date\n",
    "    # Select a few columns to verify, NOW INCLUDING CARBON\n",
    "    subset_check = df_final[df_final['date'] == check_date][['ticker', 'close', 'Oil_Price', 'Gas_Price', 'VIX_Close', 'Carbon_Price']]\n",
    "    print(f\"\\n3. Panel Check for date {check_date.date()} (Macros should be identical, Ticker/Close different):\")\n",
    "    print(subset_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c00c13-e79f-4740-a5e7-c42693f336d3",
   "metadata": {},
   "source": [
    "### 5. Final Clean-Up Code\n",
    "\n",
    "Run this block to trim those NaN rows and save the Final, Final version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "071dafa1-4b3c-41c7-8887-ec52cc949e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FINAL POLISH REPORT ---\n",
      "Original Rows: 24052\n",
      "Cleaned Rows:  23653\n",
      "Rows Dropped:  399 (Expected due to MA50 warm-up)\n",
      "Remaining Nulls: 0\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 7: The Final Polish (Trimming Warm-up NAs) ---\n",
    "\n",
    "# 1. Drop the rows with NaNs (The first ~50 days of 2010)\n",
    "df_final_clean = df_final.dropna().copy()\n",
    "\n",
    "print(\"--- FINAL POLISH REPORT ---\")\n",
    "print(f\"Original Rows: {len(df_final)}\")\n",
    "print(f\"Cleaned Rows:  {len(df_final_clean)}\")\n",
    "print(f\"Rows Dropped:  {len(df_final) - len(df_final_clean)} (Expected due to MA50 warm-up)\")\n",
    "\n",
    "# 2. Final Verify (Must be 0)\n",
    "print(f\"Remaining Nulls: {df_final_clean.isna().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d71488-f474-4865-bef5-cc717439dde9",
   "metadata": {},
   "source": [
    "### 6. Save the Master Dataset\n",
    "\n",
    "**Explanation:** Finally, we save the master_dataset.csv to your new directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8cc166d1-e739-4d5e-9bf8-098666bf8696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SUCCESS! Final Dataset saved to:\n",
      "D:\\MS_Data_Science_Thesis\\Data_Cleaning\\Master_Dataset\\master_dataset.csv\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Save the Masterpiece ---\n",
    "clean_filename = \"master_dataset.csv\"\n",
    "clean_full_path = os.path.join(final_dir, clean_filename)\n",
    "\n",
    "df_final_clean.to_csv(clean_full_path, index=False)\n",
    "print(\"=\" * 60)\n",
    "print(f\"SUCCESS! Final Dataset saved to:\\n{clean_full_path}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e00fe1-6862-4f74-ac05-5e3d2b199104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sidd_ds)",
   "language": "python",
   "name": "sidd_ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
